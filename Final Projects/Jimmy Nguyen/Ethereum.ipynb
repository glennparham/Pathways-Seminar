{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Predicting Ethereum's price with Long Short-Term Memory (LSTM) Recurrent Neural Networks \n",
    "\n",
    "\n",
    "-  Nhan (Jimmy) Nguyen\n",
    "- Data Scholar Pathways\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Recurrent Neural Networks (RNN) : \n",
    "\n",
    "- Its a powerful type of neural network designed to handle sequence dependence\n",
    "- Basicially they are networks with loops in them, allowing information to persist \n",
    "\n",
    "\n",
    "![](rnn.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "## About LSTMs\n",
    "\n",
    "\n",
    "- LTSM Network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n",
    "- They're very popular for working with sequential data such as texts, time series data etc.\n",
    "\n",
    "- **Learn more about it here:**  http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "----\n",
    "## Why use RNN?\n",
    "\n",
    "- Time Series prediction are a difficult type of predictive modeling problem.\n",
    "- Unlike regression predictive modeling, time series data adds complexity of a sequence dependence among the input variables \n",
    "---\n",
    "## Goal: \n",
    "\n",
    "1) I will learn how to a develope LSTM network using Keras deep learning library to address a demonstration of time-series prediction problem\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## Disclaimer: \n",
    "\n",
    "- There many other factors that affect cryptocurrency prices and this project is purely for educational purposes and nothing more.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About my dataset:\n",
    "\n",
    "- I want to predict the Ethereum price \n",
    "- My data consist of Ethereum data from Jan 2016-April 2018\n",
    "\n",
    "Source: https://coinmarketcap.com/currencies/ethereum/historical-data/\n",
    "\n",
    "## Why?\n",
    "\n",
    "- Because I'm tired of hearing about bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Packages \n",
    "\n",
    "\n",
    "\n",
    "- I will be using numpy for mathematical operations, pandas to operate with the csv, scikit-learn for data preprocessing and Keras with tensorflow backend as our deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "\n",
    "#keras for neural network\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "#import sklearn for modeling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Ignore this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22-Apr-18</td>\n",
       "      <td>606.12</td>\n",
       "      <td>640.77</td>\n",
       "      <td>593.87</td>\n",
       "      <td>621.86</td>\n",
       "      <td>2,426,270,000</td>\n",
       "      <td>59,985,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21-Apr-18</td>\n",
       "      <td>616</td>\n",
       "      <td>621.89</td>\n",
       "      <td>578.55</td>\n",
       "      <td>605.4</td>\n",
       "      <td>2,612,460,000</td>\n",
       "      <td>60,951,100,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20-Apr-18</td>\n",
       "      <td>567.99</td>\n",
       "      <td>618.72</td>\n",
       "      <td>560.28</td>\n",
       "      <td>615.72</td>\n",
       "      <td>2,849,470,000</td>\n",
       "      <td>56,188,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-Apr-18</td>\n",
       "      <td>524.04</td>\n",
       "      <td>567.89</td>\n",
       "      <td>523.26</td>\n",
       "      <td>567.89</td>\n",
       "      <td>2,256,870,000</td>\n",
       "      <td>51,829,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Apr-18</td>\n",
       "      <td>503.31</td>\n",
       "      <td>525.09</td>\n",
       "      <td>503.05</td>\n",
       "      <td>524.79</td>\n",
       "      <td>1,762,940,000</td>\n",
       "      <td>49,769,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17-Apr-18</td>\n",
       "      <td>511.15</td>\n",
       "      <td>518.03</td>\n",
       "      <td>502.56</td>\n",
       "      <td>502.89</td>\n",
       "      <td>1,760,360,000</td>\n",
       "      <td>50,534,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16-Apr-18</td>\n",
       "      <td>532.07</td>\n",
       "      <td>534.2</td>\n",
       "      <td>500.25</td>\n",
       "      <td>511.15</td>\n",
       "      <td>1,758,980,000</td>\n",
       "      <td>52,592,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15-Apr-18</td>\n",
       "      <td>502.88</td>\n",
       "      <td>531.7</td>\n",
       "      <td>502.88</td>\n",
       "      <td>531.7</td>\n",
       "      <td>1,726,090,000</td>\n",
       "      <td>49,696,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14-Apr-18</td>\n",
       "      <td>492.58</td>\n",
       "      <td>512.02</td>\n",
       "      <td>488.28</td>\n",
       "      <td>501.48</td>\n",
       "      <td>1,519,080,000</td>\n",
       "      <td>48,668,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13-Apr-18</td>\n",
       "      <td>493.16</td>\n",
       "      <td>526.47</td>\n",
       "      <td>482.66</td>\n",
       "      <td>492.74</td>\n",
       "      <td>2,419,250,000</td>\n",
       "      <td>48,715,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12-Apr-18</td>\n",
       "      <td>430.16</td>\n",
       "      <td>493.06</td>\n",
       "      <td>417.41</td>\n",
       "      <td>492.94</td>\n",
       "      <td>2,519,360,000</td>\n",
       "      <td>42,483,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11-Apr-18</td>\n",
       "      <td>415.02</td>\n",
       "      <td>430.54</td>\n",
       "      <td>412.47</td>\n",
       "      <td>430.54</td>\n",
       "      <td>1,439,040,000</td>\n",
       "      <td>40,980,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10-Apr-18</td>\n",
       "      <td>399.41</td>\n",
       "      <td>415.89</td>\n",
       "      <td>393.88</td>\n",
       "      <td>414.24</td>\n",
       "      <td>1,196,000,000</td>\n",
       "      <td>39,430,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9-Apr-18</td>\n",
       "      <td>400.86</td>\n",
       "      <td>429.25</td>\n",
       "      <td>390.61</td>\n",
       "      <td>398.53</td>\n",
       "      <td>1,478,390,000</td>\n",
       "      <td>39,565,100,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8-Apr-18</td>\n",
       "      <td>385.74</td>\n",
       "      <td>402.59</td>\n",
       "      <td>385.6</td>\n",
       "      <td>400.51</td>\n",
       "      <td>948,488,000</td>\n",
       "      <td>38,065,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7-Apr-18</td>\n",
       "      <td>370.38</td>\n",
       "      <td>393.06</td>\n",
       "      <td>369.94</td>\n",
       "      <td>385.31</td>\n",
       "      <td>951,475,000</td>\n",
       "      <td>36,541,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6-Apr-18</td>\n",
       "      <td>382.73</td>\n",
       "      <td>385.2</td>\n",
       "      <td>366.91</td>\n",
       "      <td>370.29</td>\n",
       "      <td>967,106,000</td>\n",
       "      <td>37,752,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5-Apr-18</td>\n",
       "      <td>379.95</td>\n",
       "      <td>387.72</td>\n",
       "      <td>369.82</td>\n",
       "      <td>383.23</td>\n",
       "      <td>1,210,680,000</td>\n",
       "      <td>37,470,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4-Apr-18</td>\n",
       "      <td>416.49</td>\n",
       "      <td>417.47</td>\n",
       "      <td>375.31</td>\n",
       "      <td>380.54</td>\n",
       "      <td>1,287,730,000</td>\n",
       "      <td>41,065,100,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3-Apr-18</td>\n",
       "      <td>387.31</td>\n",
       "      <td>418.97</td>\n",
       "      <td>383.53</td>\n",
       "      <td>416.89</td>\n",
       "      <td>1,363,400,000</td>\n",
       "      <td>38,180,800,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2-Apr-18</td>\n",
       "      <td>379.7</td>\n",
       "      <td>395.17</td>\n",
       "      <td>377.59</td>\n",
       "      <td>386.43</td>\n",
       "      <td>1,102,260,000</td>\n",
       "      <td>37,422,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1-Apr-18</td>\n",
       "      <td>397.25</td>\n",
       "      <td>400.53</td>\n",
       "      <td>363.81</td>\n",
       "      <td>379.61</td>\n",
       "      <td>1,256,930,000</td>\n",
       "      <td>39,144,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31-Mar-18</td>\n",
       "      <td>395</td>\n",
       "      <td>418.47</td>\n",
       "      <td>392.95</td>\n",
       "      <td>396.46</td>\n",
       "      <td>1,323,920,000</td>\n",
       "      <td>38,914,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30-Mar-18</td>\n",
       "      <td>385.91</td>\n",
       "      <td>409.93</td>\n",
       "      <td>368.63</td>\n",
       "      <td>394.65</td>\n",
       "      <td>1,878,130,000</td>\n",
       "      <td>38,010,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29-Mar-18</td>\n",
       "      <td>448.08</td>\n",
       "      <td>450.81</td>\n",
       "      <td>385.81</td>\n",
       "      <td>385.97</td>\n",
       "      <td>1,970,230,000</td>\n",
       "      <td>44,125,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28-Mar-18</td>\n",
       "      <td>450.29</td>\n",
       "      <td>466.21</td>\n",
       "      <td>444.86</td>\n",
       "      <td>446.28</td>\n",
       "      <td>1,514,180,000</td>\n",
       "      <td>44,334,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27-Mar-18</td>\n",
       "      <td>489.59</td>\n",
       "      <td>491.46</td>\n",
       "      <td>449.97</td>\n",
       "      <td>450.12</td>\n",
       "      <td>1,617,940,000</td>\n",
       "      <td>48,193,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26-Mar-18</td>\n",
       "      <td>524.29</td>\n",
       "      <td>526.38</td>\n",
       "      <td>470.44</td>\n",
       "      <td>489.95</td>\n",
       "      <td>1,638,880,000</td>\n",
       "      <td>51,598,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25-Mar-18</td>\n",
       "      <td>522.7</td>\n",
       "      <td>535.82</td>\n",
       "      <td>515.66</td>\n",
       "      <td>524.29</td>\n",
       "      <td>1,151,170,000</td>\n",
       "      <td>51,431,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24-Mar-18</td>\n",
       "      <td>542.57</td>\n",
       "      <td>545.38</td>\n",
       "      <td>526.08</td>\n",
       "      <td>526.44</td>\n",
       "      <td>1,300,010,000</td>\n",
       "      <td>53,375,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>30-Jan-16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3,725,080</td>\n",
       "      <td>192,176,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>29-Jan-16</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.49</td>\n",
       "      <td>6,662,340</td>\n",
       "      <td>194,815,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>28-Jan-16</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.53</td>\n",
       "      <td>4,903,850</td>\n",
       "      <td>184,294,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>27-Jan-16</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.39</td>\n",
       "      <td>6,961,080</td>\n",
       "      <td>172,061,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>26-Jan-16</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.28</td>\n",
       "      <td>13,939,200</td>\n",
       "      <td>193,838,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>25-Jan-16</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10,739,300</td>\n",
       "      <td>164,313,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>24-Jan-16</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.14</td>\n",
       "      <td>9,669,770</td>\n",
       "      <td>145,358,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>23-Jan-16</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.97</td>\n",
       "      <td>7,370,980</td>\n",
       "      <td>115,002,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>22-Jan-16</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1,567,240</td>\n",
       "      <td>118,244,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>21-Jan-16</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1,614,960</td>\n",
       "      <td>117,111,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>20-Jan-16</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2,933,220</td>\n",
       "      <td>104,124,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>19-Jan-16</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2,446,740</td>\n",
       "      <td>108,628,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>18-Jan-16</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.43</td>\n",
       "      <td>4,278,930</td>\n",
       "      <td>101,699,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>17-Jan-16</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1,171,150</td>\n",
       "      <td>93,207,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>16-Jan-16</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2,462,630</td>\n",
       "      <td>93,099,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>15-Jan-16</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3,910,550</td>\n",
       "      <td>90,321,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>14-Jan-16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>751,961</td>\n",
       "      <td>85,539,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>13-Jan-16</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1,005,910</td>\n",
       "      <td>86,530,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>12-Jan-16</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2,898,760</td>\n",
       "      <td>81,217,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>11-Jan-16</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1,011,920</td>\n",
       "      <td>76,096,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>10-Jan-16</td>\n",
       "      <td>0.985557</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975099</td>\n",
       "      <td>0.999231</td>\n",
       "      <td>390,888</td>\n",
       "      <td>75,031,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>9-Jan-16</td>\n",
       "      <td>0.985501</td>\n",
       "      <td>0.992345</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.986833</td>\n",
       "      <td>226,281</td>\n",
       "      <td>75,001,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>8-Jan-16</td>\n",
       "      <td>0.942752</td>\n",
       "      <td>0.991825</td>\n",
       "      <td>0.939715</td>\n",
       "      <td>0.986789</td>\n",
       "      <td>545,600</td>\n",
       "      <td>71,724,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>7-Jan-16</td>\n",
       "      <td>0.955801</td>\n",
       "      <td>0.974623</td>\n",
       "      <td>0.93583</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>647,462</td>\n",
       "      <td>72,692,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>6-Jan-16</td>\n",
       "      <td>0.950028</td>\n",
       "      <td>0.960659</td>\n",
       "      <td>0.935708</td>\n",
       "      <td>0.95086</td>\n",
       "      <td>308,791</td>\n",
       "      <td>72,229,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>5-Jan-16</td>\n",
       "      <td>0.953147</td>\n",
       "      <td>0.970597</td>\n",
       "      <td>0.946543</td>\n",
       "      <td>0.950176</td>\n",
       "      <td>219,833</td>\n",
       "      <td>72,442,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>4-Jan-16</td>\n",
       "      <td>0.972045</td>\n",
       "      <td>0.976438</td>\n",
       "      <td>0.929835</td>\n",
       "      <td>0.95448</td>\n",
       "      <td>346,245</td>\n",
       "      <td>73,853,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>3-Jan-16</td>\n",
       "      <td>0.93843</td>\n",
       "      <td>0.991362</td>\n",
       "      <td>0.934313</td>\n",
       "      <td>0.971905</td>\n",
       "      <td>407,632</td>\n",
       "      <td>71,275,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>2-Jan-16</td>\n",
       "      <td>0.947401</td>\n",
       "      <td>0.969637</td>\n",
       "      <td>0.93656</td>\n",
       "      <td>0.937124</td>\n",
       "      <td>255,504</td>\n",
       "      <td>71,933,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>1-Jan-16</td>\n",
       "      <td>0.933712</td>\n",
       "      <td>0.954822</td>\n",
       "      <td>0.931442</td>\n",
       "      <td>0.948024</td>\n",
       "      <td>206,062</td>\n",
       "      <td>70,870,900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Open      High       Low     Close         Volume  \\\n",
       "0    22-Apr-18    606.12    640.77    593.87    621.86  2,426,270,000   \n",
       "1    21-Apr-18       616    621.89    578.55     605.4  2,612,460,000   \n",
       "2    20-Apr-18    567.99    618.72    560.28    615.72  2,849,470,000   \n",
       "3    19-Apr-18    524.04    567.89    523.26    567.89  2,256,870,000   \n",
       "4    18-Apr-18    503.31    525.09    503.05    524.79  1,762,940,000   \n",
       "5    17-Apr-18    511.15    518.03    502.56    502.89  1,760,360,000   \n",
       "6    16-Apr-18    532.07     534.2    500.25    511.15  1,758,980,000   \n",
       "7    15-Apr-18    502.88     531.7    502.88     531.7  1,726,090,000   \n",
       "8    14-Apr-18    492.58    512.02    488.28    501.48  1,519,080,000   \n",
       "9    13-Apr-18    493.16    526.47    482.66    492.74  2,419,250,000   \n",
       "10   12-Apr-18    430.16    493.06    417.41    492.94  2,519,360,000   \n",
       "11   11-Apr-18    415.02    430.54    412.47    430.54  1,439,040,000   \n",
       "12   10-Apr-18    399.41    415.89    393.88    414.24  1,196,000,000   \n",
       "13    9-Apr-18    400.86    429.25    390.61    398.53  1,478,390,000   \n",
       "14    8-Apr-18    385.74    402.59     385.6    400.51    948,488,000   \n",
       "15    7-Apr-18    370.38    393.06    369.94    385.31    951,475,000   \n",
       "16    6-Apr-18    382.73     385.2    366.91    370.29    967,106,000   \n",
       "17    5-Apr-18    379.95    387.72    369.82    383.23  1,210,680,000   \n",
       "18    4-Apr-18    416.49    417.47    375.31    380.54  1,287,730,000   \n",
       "19    3-Apr-18    387.31    418.97    383.53    416.89  1,363,400,000   \n",
       "20    2-Apr-18     379.7    395.17    377.59    386.43  1,102,260,000   \n",
       "21    1-Apr-18    397.25    400.53    363.81    379.61  1,256,930,000   \n",
       "22   31-Mar-18       395    418.47    392.95    396.46  1,323,920,000   \n",
       "23   30-Mar-18    385.91    409.93    368.63    394.65  1,878,130,000   \n",
       "24   29-Mar-18    448.08    450.81    385.81    385.97  1,970,230,000   \n",
       "25   28-Mar-18    450.29    466.21    444.86    446.28  1,514,180,000   \n",
       "26   27-Mar-18    489.59    491.46    449.97    450.12  1,617,940,000   \n",
       "27   26-Mar-18    524.29    526.38    470.44    489.95  1,638,880,000   \n",
       "28   25-Mar-18     522.7    535.82    515.66    524.29  1,151,170,000   \n",
       "29   24-Mar-18    542.57    545.38    526.08    526.44  1,300,010,000   \n",
       "..         ...       ...       ...       ...       ...            ...   \n",
       "813  30-Jan-16      2.51      2.61       2.4      2.45      3,725,080   \n",
       "814  29-Jan-16      2.54      2.61       2.3      2.49      6,662,340   \n",
       "815  28-Jan-16      2.41      2.68      2.34      2.53      4,903,850   \n",
       "816  27-Jan-16      2.25      2.59      2.19      2.39      6,961,080   \n",
       "817  26-Jan-16      2.53      2.84      1.99      2.28     13,939,200   \n",
       "818  25-Jan-16      2.15      2.66      2.06       2.5     10,739,300   \n",
       "819  24-Jan-16       1.9       2.3       1.9      2.14      9,669,770   \n",
       "820  23-Jan-16       1.5      2.03       1.5      1.97      7,370,980   \n",
       "821  22-Jan-16      1.55      1.58      1.47       1.5      1,567,240   \n",
       "822  21-Jan-16      1.53      1.59      1.49      1.55      1,614,960   \n",
       "823  20-Jan-16      1.36      1.62      1.35      1.53      2,933,220   \n",
       "824  19-Jan-16      1.42      1.46      1.31      1.37      2,446,740   \n",
       "825  18-Jan-16      1.33      1.54      1.32      1.43      4,278,930   \n",
       "826  17-Jan-16      1.22      1.33      1.21      1.33      1,171,150   \n",
       "827  16-Jan-16      1.22      1.31      1.17      1.22      2,462,630   \n",
       "828  15-Jan-16      1.18      1.32      1.14      1.21      3,910,550   \n",
       "829  14-Jan-16      1.12      1.19       1.1      1.19        751,961   \n",
       "830  13-Jan-16      1.14      1.15      1.08      1.13      1,005,910   \n",
       "831  12-Jan-16      1.07      1.28      1.05      1.14      2,898,760   \n",
       "832  11-Jan-16  0.999216      1.07  0.999216      1.06      1,011,920   \n",
       "833  10-Jan-16  0.985557         1  0.975099  0.999231        390,888   \n",
       "834   9-Jan-16  0.985501  0.992345    0.9736  0.986833        226,281   \n",
       "835   8-Jan-16  0.942752  0.991825  0.939715  0.986789        545,600   \n",
       "836   7-Jan-16  0.955801  0.974623   0.93583  0.942005        647,462   \n",
       "837   6-Jan-16  0.950028  0.960659  0.935708   0.95086        308,791   \n",
       "838   5-Jan-16  0.953147  0.970597  0.946543  0.950176        219,833   \n",
       "839   4-Jan-16  0.972045  0.976438  0.929835   0.95448        346,245   \n",
       "840   3-Jan-16   0.93843  0.991362  0.934313  0.971905        407,632   \n",
       "841   2-Jan-16  0.947401  0.969637   0.93656  0.937124        255,504   \n",
       "842   1-Jan-16  0.933712  0.954822  0.931442  0.948024        206,062   \n",
       "\n",
       "         Market Cap  \n",
       "0    59,985,500,000  \n",
       "1    60,951,100,000  \n",
       "2    56,188,700,000  \n",
       "3    51,829,900,000  \n",
       "4    49,769,600,000  \n",
       "5    50,534,000,000  \n",
       "6    52,592,200,000  \n",
       "7    49,696,300,000  \n",
       "8    48,668,400,000  \n",
       "9    48,715,400,000  \n",
       "10   42,483,600,000  \n",
       "11   40,980,200,000  \n",
       "12   39,430,400,000  \n",
       "13   39,565,100,000  \n",
       "14   38,065,400,000  \n",
       "15   36,541,900,000  \n",
       "16   37,752,600,000  \n",
       "17   37,470,200,000  \n",
       "18   41,065,100,000  \n",
       "19   38,180,800,000  \n",
       "20   37,422,500,000  \n",
       "21   39,144,700,000  \n",
       "22   38,914,900,000  \n",
       "23   38,010,600,000  \n",
       "24   44,125,000,000  \n",
       "25   44,334,000,000  \n",
       "26   48,193,300,000  \n",
       "27   51,598,000,000  \n",
       "28   51,431,600,000  \n",
       "29   53,375,400,000  \n",
       "..              ...  \n",
       "813     192,176,000  \n",
       "814     194,815,000  \n",
       "815     184,294,000  \n",
       "816     172,061,000  \n",
       "817     193,838,000  \n",
       "818     164,313,000  \n",
       "819     145,358,000  \n",
       "820     115,002,000  \n",
       "821     118,244,000  \n",
       "822     117,111,000  \n",
       "823     104,124,000  \n",
       "824     108,628,000  \n",
       "825     101,699,000  \n",
       "826      93,207,300  \n",
       "827      93,099,400  \n",
       "828      90,321,700  \n",
       "829      85,539,300  \n",
       "830      86,530,000  \n",
       "831      81,217,300  \n",
       "832      76,096,500  \n",
       "833      75,031,200  \n",
       "834      75,001,800  \n",
       "835      71,724,000  \n",
       "836      72,692,900  \n",
       "837      72,229,700  \n",
       "838      72,442,400  \n",
       "839      73,853,900  \n",
       "840      71,275,900  \n",
       "841      71,933,100  \n",
       "842      70,870,900  \n",
       "\n",
       "[843 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ethereum.csv')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "- We don't need the following columns:\n",
    "\n",
    "        Date, Open, High, Low, Volume, and Market Cap\n",
    " \n",
    "- Instead we want to convert our single column **Close** to a two-column dataset where it contains today's Ethereum price (t) and the second column contraining tomorrow's (t + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22-Apr-18</td>\n",
       "      <td>606.12</td>\n",
       "      <td>640.77</td>\n",
       "      <td>593.87</td>\n",
       "      <td>621.86</td>\n",
       "      <td>2426270000</td>\n",
       "      <td>59985500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21-Apr-18</td>\n",
       "      <td>616</td>\n",
       "      <td>621.89</td>\n",
       "      <td>578.55</td>\n",
       "      <td>605.4</td>\n",
       "      <td>2612460000</td>\n",
       "      <td>60951100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20-Apr-18</td>\n",
       "      <td>567.99</td>\n",
       "      <td>618.72</td>\n",
       "      <td>560.28</td>\n",
       "      <td>615.72</td>\n",
       "      <td>2849470000</td>\n",
       "      <td>56188700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-Apr-18</td>\n",
       "      <td>524.04</td>\n",
       "      <td>567.89</td>\n",
       "      <td>523.26</td>\n",
       "      <td>567.89</td>\n",
       "      <td>2256870000</td>\n",
       "      <td>51829900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Apr-18</td>\n",
       "      <td>503.31</td>\n",
       "      <td>525.09</td>\n",
       "      <td>503.05</td>\n",
       "      <td>524.79</td>\n",
       "      <td>1762940000</td>\n",
       "      <td>49769600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date    Open    High     Low   Close      Volume   Market Cap\n",
       "0  22-Apr-18  606.12  640.77  593.87  621.86  2426270000  59985500000\n",
       "1  21-Apr-18     616  621.89  578.55   605.4  2612460000  60951100000\n",
       "2  20-Apr-18  567.99  618.72  560.28  615.72  2849470000  56188700000\n",
       "3  19-Apr-18  524.04  567.89  523.26  567.89  2256870000  51829900000\n",
       "4  18-Apr-18  503.31  525.09  503.05  524.79  1762940000  49769600000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Date', 'Open','High', 'Low', 'Close', 'Volume','Market Cap']\n",
    "\n",
    "#replacing commas\n",
    "df[cols] = df[cols].replace({',': ''}, regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets drop the unnecessary columns. \n",
    "- Then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df =  df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Date','Open','High','Low','Volume','Market Cap'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "dataset=dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTMs are sensitive to the scale of the input data. \n",
    "- So generally we should rescale the data to the range of 0-to-1 (**normalizing**). \n",
    "- We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Now lets create our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the following function will take in a numpy array where we want to convert into a dataset. \n",
    "- Our X variable will be the closing price of Ethereum at a given time (t) and Y will be the closing price of Ethereum at the next (t+1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset):\n",
    "    X_data, y_data = [], []\n",
    "    for i in range(len(dataset)-1):\n",
    "        X_data.append(dataset[i])\n",
    "        y_data.append(dataset[i + 1])\n",
    "    return np.asarray(X_data), np.asarray(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating our X and Y \n",
    "X,y = make_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Now lets take 80% of the data as training set and the rest (20%) as our testing set by using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting our \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "###  For the LSTM network, we want to format the shape of the input data X as \n",
    "     [samples, time steps, features]  \n",
    "\n",
    "### Right now the shape of our data is in the form of \n",
    "\n",
    "        [samples, features]\n",
    "        \n",
    "### We can transform the train and test data into the desired structure by using \n",
    "\n",
    "        np.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Creating our model \n",
    "\n",
    "- Now we will design the LSTM model for our problem.\n",
    "- The network will have\n",
    "    - 1 input layer\n",
    "    - 1 hidden layer with 4 LSTM blocks. \n",
    "    \n",
    "     The LSTM blocks will use  sigmoid activation function by default.\n",
    "    - 1 Output layer - to make a single value prediction\n",
    " \n",
    "- We are training the network for 5 epochs and a batch size of 1  \n",
    "\n",
    "\n",
    "--------\n",
    "* **In the neural network terminology:**\n",
    "    - **one epoch** = one forward pass and one backward pass of all the training examples\n",
    "    - **batch size** = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "\n",
    "    - **number of iterations** = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "    - **Example:** if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 2s - loss: 0.0080\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.0049\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 4/5\n",
      " - 2s - loss: 5.3945e-05\n",
      "Epoch 5/5\n",
      " - 2s - loss: 5.2287e-05\n"
     ]
    }
   ],
   "source": [
    "# create LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, 1)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#MSE \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Fit the model \n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1, verbose=2)\n",
    "#save model for later use\n",
    "model.save('Model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load_model\n",
    "model = load_model('Model2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "## Estimating the performance of our model on the train and test datasets after fitting the model.\n",
    "\n",
    "- The performance estimation will give us a point of comparison for new models\n",
    "\n",
    "- Before calculating the error scores, we want to invert the predictions of so that our performance is reported in the same units as the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "future_predict = model.predict(np.asarray([[test_predict[-1]]]))\n",
    "future_predict = scaler.inverse_transform(future_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "y_train = scaler.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "y_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Now we can use the model to predict the price for tomorrow\n",
    "- Using the model we trained earlier and pass in today's price as the parameter where it would be the last row in the test dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing Price of Ethereum for last 5 days: \n",
      "[[519.2338 ]\n",
      " [544.02594]\n",
      " [593.0836 ]\n",
      " [647.8555 ]\n",
      " [636.01337]]\n",
      "Ethereum price for tomorrow:  [[671.1738]]\n",
      "Train Score: 10.05 RMSE\n",
      "Test Score: 79.62 RMSE\n"
     ]
    }
   ],
   "source": [
    "print(\"Closing Price of Ethereum for last 5 days: \")\n",
    "print(test_predict[-5:])\n",
    "\n",
    "print(\"Ethereum price for tomorrow: \", future_predict)\n",
    "\n",
    "\n",
    "\n",
    "# calculating RMSE for our training dataset\n",
    "train_score = math.sqrt(mean_squared_error(y_train[:,0], train_predict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (train_score))\n",
    "\n",
    "\n",
    "# calculating RMSE for our test dataset \n",
    "test_score = math.sqrt(mean_squared_error(y_test[:,0], test_predict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e34d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[1:len(train_predict)+1, :] = train_predict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict):len(dataset)-1, :] = test_predict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
